# imports
import tensorflow as tf
import tensorflow_probability as tfp

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import RMSprop
from keras import regularizers


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
import pickle
from scipy.stats import multivariate_normal
import os
import matplotlib.pyplot as plt

tfd = tfp.distributions
tfpl = tfp.layers



#Evaluation Functions 
def nll(y_true, y_pred):
    """
    This function should return the negative log-likelihood of each sample
    in y_true given the predicted distribution y_pred. If y_true is of shape
    [B, E] and y_pred has batch shape [B] and event_shape [E], the output
    should be a Tensor of shape [B].
    """
    return -y_pred.log_prob(y_true)

# Uncertainty quantification using entropy

def get_correct_indices(model, x, labels):
    y_model = model(x)
    correct = np.argmax(y_model.mean(), axis=1) == np.squeeze(labels)
    correct_indices = [i for i in range(x.shape[0]) if correct[i]]
    incorrect_indices = [i for i in range(x.shape[0]) if not correct[i]]
    return correct_indices, incorrect_indices


def plot_entropy_distribution(model, x, labels):
    probs = model(x).mean().numpy()
    entropy = -np.sum(probs * np.log2(probs), axis=1)
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    for i, category in zip(range(2), ['Correct', 'Incorrect']):
      print(entropy)

      entropy_category = entropy[get_correct_indices(model, x, labels)[i]]
      mean_entropy = np.mean(entropy_category)
      num_samples = entropy_category.shape[0]
      title = category + 'ly labelled ({:.1f}% of total)'.format(num_samples / x.shape[0] * 100)
      axes[i].hist(entropy_category, weights=(1/num_samples)*np.ones(num_samples))
      axes[i].annotate('Mean: {:.3f} bits'.format(mean_entropy), (0.4, 0.9), ha='center')
      axes[i].set_xlabel('Entropy (bits)')
      axes[i].set_ylim([0, 1])
      axes[i].set_ylabel('Probability')
      axes[i].set_title(title)
    plt.show()



#  Custom prior
def spike_and_slab(event_shape, dtype):
    distribution = tfd.Mixture(
        cat=tfd.Categorical(probs=[0.5, 0.5]),
        components=[
            tfd.Independent(tfd.Normal(
                loc=tf.zeros(event_shape, dtype=dtype),
                scale=0.1*tf.ones(event_shape, dtype=dtype)),
                            reinterpreted_batch_ndims=1),
            tfd.Independent(tfd.Normal(
                loc=tf.zeros(event_shape, dtype=dtype),
                scale=1.0*tf.ones(event_shape, dtype=dtype)),
                            reinterpreted_batch_ndims=1)],
    name='spike_and_slab')
    return distribution


x_plot = np.linspace(-5, 5, 1000)[:, np.newaxis]
plt.plot(x_plot, tfd.Normal(loc=0, scale=1).prob(x_plot).numpy(), label='unit normal', linestyle='--')
plt.plot(x_plot, spike_and_slab(1, dtype=tf.float32).prob(x_plot).numpy(), label='spike and slab')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.show()




#Model
def get_dense_reparameterization_layer(input_shape, divergence_fn, units):
    """
    This function should create an instance of a Convolution2DReparameterization
    layer according to the above specification.
    The function takes the input_shape and divergence_fn as arguments, which should
    be used to define the layer.
    Your function should then return the layer instance.
    """

    layer = tfpl.DenseReparameterization(
                input_shape= input_shape, units= units,
                activation='swish',

                kernel_prior_fn= tfpl.default_mean_field_normal_fn(is_singular=False),       #tfpl.default_multivariate_normal_fn,
                kernel_posterior_fn= tfpl.default_mean_field_normal_fn(is_singular=False),
                kernel_divergence_fn= divergence_fn,

                bias_prior_fn= tfpl.default_mean_field_normal_fn(is_singular=False),
                bias_posterior_fn= tfpl.default_mean_field_normal_fn(is_singular=False),
                bias_divergence_fn= divergence_fn
            )
    return layer


def get_prior(kernel_size, bias_size, dtype=None):
    """
    This function should create the prior distribution, consisting of the
    "spike and slab" distribution that is described above.
    The distribution should be created using the kernel_size, bias_size and dtype
    function arguments above.
    The function should then return a callable, that returns the prior distribution.
    """
    n = kernel_size+bias_size
    print("prior",n)

    # prior_model = Sequential([tfpl.DistributionLambda(lambda t : spike_and_slab(n, dtype))])        #prior (should change and compare)
    # prior_model = Sequential([tfpl.DistributionLambda(lambda t : tfd.Horseshoe(scale= 0.1*tf.ones(n, dtype)))])    # this is working weakly

    # prior_model = Sequential([tfpl.DistributionLambda(lambda t : tfp.layers.MultivariateNormalTriL(n, convert_to_tensor_fn= cos_mul.sample))])        #prior (should change and compare)
    # return prior_model


    return Sequential([
        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),     # trainble prior
        tfpl.IndependentNormal(n)
    ])

def get_posterior(kernel_size, bias_size, dtype=None):
    """
    This function should create the posterior distribution as specified above.
    The distribution should be created using the kernel_size, bias_size and dtype
    function arguments above.
    The function should then return a callable, that returns the posterior distribution.
    """
    n = kernel_size + bias_size
    print("post",n)
    return Sequential([
        tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n), dtype=dtype),     #*********** tfpl.MultivariateNormalTriL.params_size(n))  #tfpl.IndependentNormal.params_size
        tfpl.IndependentNormal(n)
    ])

def get_dense_variational_layer(prior_fn, posterior_fn, kl_weight):
    """
    This function should create an instance of a DenseVariational layer according
    to the above specification.
    The function takes the prior_fn, posterior_fn and kl_weight as arguments, which should
    be used to define the layer.
    Your function should then return the layer instance.
    """
    return tfpl.DenseVariational(
        units=31, make_posterior_fn=posterior_fn, make_prior_fn=prior_fn, kl_weight=kl_weight
    )

tf.random.set_seed(0)
divergence_fn = lambda q, p, _ : tfd.kl_divergence(q, p) / X_train.shape[0]


from ast import Return

def bys_model():

    bayesian_model = Sequential([tf.keras.layers.InputLayer(input_shape=(100,)),
        tf.keras.layers.Dense(128, activation='swish', bias_regularizer=regularizers.L2(1e-4)),
        Dropout(0.1),
        tf.keras.layers.Dense(64, activation='swish', bias_regularizer=regularizers.L2(1e-4)),    #, kernel_regularizer=regularizers.l1_l2(0.001)
        tf.keras.layers.Dense(32, activation='swish', bias_regularizer=regularizers.L2(1e-4)),    #, bias_regularizer=regularizers.L2(1e-4)

        get_dense_reparameterization_layer((100,), divergence_fn, 64),
        Dropout(0.1),
        get_dense_reparameterization_layer((100,), divergence_fn, 64),

        Flatten(),

        # tfp.layers.DenseReparameterization(128, activation=tf.nn.relu),
        # Dropout(0.1),
        # tfp.layers.DenseReparameterization(64, activation=tf.nn.relu),


        get_dense_variational_layer(get_prior, get_posterior, kl_weight= 1/X_train.shape[0] * 0.5),  # this is where costume prior works(for aleatoric) *kl_weight can be changed to   1/x.shape[0] * 0.5
        tfpl.OneHotCategorical(31, convert_to_tensor_fn= tfd.Distribution.mode)
    ])
    bayesian_model.compile(loss=nll,
                  optimizer= tf.keras.optimizers.Adam(0.001),
                  metrics=['accuracy'],
                  experimental_run_tf_function=False)

    bayesian_model.summary()
    return bayesian_model


model = bys_model()
history = model.fit(x= X_train, y= y_train_oh, epochs= 30, verbose=True, validation_data = (X_val, y_val_oh))  #, callbacks=[EarlyStopping(monitor='val_loss')]


# Plot the training and validation curves

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()


print('Accuracy on test set: ',
      str(model.evaluate(X_test, y_test_oh, verbose=False)[1]))
